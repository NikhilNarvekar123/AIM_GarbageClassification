{"cells":[{"source":["## This is the one with 6 classes of cardboard, glass, metal , paper, plastic and trash.\n","https://www.kaggle.com/asdasdasasdas/garbage-classification\n","### Kaggle Notebook\n","https://www.kaggle.com/yokidcoolkid/ais-notebook-2-electric-boogaloo"],"cell_type":"markdown","metadata":{}},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import pathlib\n","import random\n","import glob\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import preprocessing\n","from tensorflow.keras.preprocessing import image_dataset_from_directory\n","from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n","from tensorflow.keras import layers\n","import PIL\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg"],"execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["images_path = os.path.join(os.path.realpath('..'), \"input\", \"garbage-classification\", \"Garbage classification\", \"Garbage classification\")\n","images_path = pathlib.Path(images_path)\n","\n","total = len(list(images_path.glob('*/*.jpg')))\n","\n","print(f\"total images: {total}\")\n","\n","batch_size = 128\n","img_height = 180\n","img_width = 180\n","rescale = Rescaling(scale=1.0/255)"],"execution_count":19,"outputs":[{"output_type":"stream","text":"total images: 2527\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":["train_ds = image_dataset_from_directory(\n","  images_path,\n","  validation_split=0.2,\n","  subset=\"training\",\n","  seed=1232376,\n","  image_size=(img_height, img_width),\n","  batch_size=batch_size)\n","\n","train_ds = train_ds.map(lambda image,label:(rescale(image),label))"],"execution_count":20,"outputs":[{"output_type":"stream","text":"Found 2527 files belonging to 6 classes.\nUsing 2022 files for training.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":["val_ds = image_dataset_from_directory(\n","  images_path,\n","  validation_split=0.2,\n","  subset=\"validation\",\n","  seed=1232376,\n","  image_size=(img_height, img_width),\n","  batch_size=batch_size)\n","\n","val_ds = val_ds.map(lambda image,label:(rescale(image),label))"],"execution_count":21,"outputs":[{"output_type":"stream","text":"Found 2527 files belonging to 6 classes.\nUsing 505 files for validation.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":["num_classes = 6\n","\n","model = tf.keras.Sequential([\n","  layers.Conv2D(32, 3, activation='relu'),\n","  layers.MaxPooling2D(),\n","  layers.Conv2D(32, 3, activation='relu'),\n","  layers.MaxPooling2D(),\n","  layers.Conv2D(32, 3, activation='relu'),\n","  layers.MaxPooling2D(),\n","  layers.Flatten(),\n","  layers.Dense(128, activation='relu'),\n","  layers.Dense(num_classes)\n","])\n","\n","model.compile(\n","  optimizer='adam',\n","  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n","  metrics=['accuracy'])"],"execution_count":25,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'metrics' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-8da47c97dbd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseCategoricalCrossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_logits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m   metrics=['accuracy', metrics.val_f1s])\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'metrics' is not defined"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":["model.fit(\n","  train_ds,\n","  validation_data=val_ds,\n","  epochs=10\n",")\n","\n","model.summary()"],"execution_count":24,"outputs":[{"output_type":"stream","text":"Epoch 1/10\n16/16 [==============================] - 6s 250ms/step - loss: 1.3454 - accuracy: 0.4654 - val_loss: 1.3533 - val_accuracy: 0.4752\nEpoch 2/10\n16/16 [==============================] - 6s 255ms/step - loss: 1.2251 - accuracy: 0.5114 - val_loss: 1.3225 - val_accuracy: 0.5129\nEpoch 3/10\n16/16 [==============================] - 6s 253ms/step - loss: 1.0971 - accuracy: 0.5702 - val_loss: 1.2366 - val_accuracy: 0.5604\nEpoch 4/10\n16/16 [==============================] - 6s 257ms/step - loss: 1.0102 - accuracy: 0.6162 - val_loss: 1.2110 - val_accuracy: 0.5505\nEpoch 5/10\n16/16 [==============================] - 6s 249ms/step - loss: 0.9145 - accuracy: 0.6494 - val_loss: 1.2012 - val_accuracy: 0.5505\nEpoch 6/10\n16/16 [==============================] - 6s 283ms/step - loss: 0.7663 - accuracy: 0.7211 - val_loss: 1.2351 - val_accuracy: 0.5663\nEpoch 7/10\n16/16 [==============================] - 5s 246ms/step - loss: 0.7109 - accuracy: 0.7384 - val_loss: 1.3133 - val_accuracy: 0.5426\nEpoch 8/10\n16/16 [==============================] - 6s 259ms/step - loss: 0.6794 - accuracy: 0.7512 - val_loss: 1.1953 - val_accuracy: 0.5921\nEpoch 9/10\n16/16 [==============================] - 6s 247ms/step - loss: 0.5248 - accuracy: 0.8220 - val_loss: 1.3155 - val_accuracy: 0.5901\nEpoch 10/10\n16/16 [==============================] - 6s 256ms/step - loss: 0.4290 - accuracy: 0.8640 - val_loss: 1.2693 - val_accuracy: 0.5941\nModel: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_3 (Conv2D)            (None, 178, 178, 32)      896       \n_________________________________________________________________\nmax_pooling2d_3 (MaxPooling2 (None, 89, 89, 32)        0         \n_________________________________________________________________\nconv2d_4 (Conv2D)            (None, 87, 87, 32)        9248      \n_________________________________________________________________\nmax_pooling2d_4 (MaxPooling2 (None, 43, 43, 32)        0         \n_________________________________________________________________\nconv2d_5 (Conv2D)            (None, 41, 41, 32)        9248      \n_________________________________________________________________\nmax_pooling2d_5 (MaxPooling2 (None, 20, 20, 32)        0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 12800)             0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 128)               1638528   \n_________________________________________________________________\ndense_3 (Dense)              (None, 6)                 774       \n=================================================================\nTotal params: 1,658,694\nTrainable params: 1,658,694\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}